{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available scene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "f = open('../../01/01_scene_list/scene.json', \"r\")\n",
    "scene_json = json.load(f)\n",
    "\n",
    "# Available scene list\n",
    "scene_list = list(scene_json.keys())\n",
    "\n",
    "# Coordinate conversion\n",
    "scene_dict = scene_json\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_scene_df(scene_id):\n",
    "    \n",
    "    ds_pth = \"E:\\datasets\\iGibson-dataset\\position_rd_no_obj/\"\n",
    "    ds_pth = f\"{ds_pth}/{scene_id}/\"\n",
    "    sp_nums = len(os.listdir(ds_pth))\n",
    "    \n",
    "    mp_pth = \"E:\\datasets\\iGibson-dataset\\scene_map_processed/\"\n",
    "    mp_pth = f\"{mp_pth}/{scene_id}/{scene_id}.png\"\n",
    "    \n",
    "    tgt_pth = f\"E:\\datasets\\iGibson-dataset\\position_csv/\"\n",
    "    tgt_pth = f\"{tgt_pth}/{scene_id}.csv\"\n",
    "    \n",
    "    scene_df = {\n",
    "        \"gt\": [],\n",
    "        \"rgb\": [],\n",
    "        \"d\": [],\n",
    "        \"mp\": []\n",
    "    }\n",
    "    \n",
    "    for idx in range(sp_nums):\n",
    "        \n",
    "        gt_pth = f\"{ds_pth}/{idx}/gt/\"\n",
    "        scene_df[\"gt\"].append(gt_pth)\n",
    "        \n",
    "        rgb_pth = f\"{ds_pth}/{idx}/rgb/\"\n",
    "        scene_df[\"rgb\"].append(rgb_pth)\n",
    "        \n",
    "        d_pth = f\"{ds_pth}/{idx}/d/\"\n",
    "        scene_df[\"d\"].append(d_pth)\n",
    "        \n",
    "        scene_df[\"mp\"].append(mp_pth)\n",
    "        \n",
    "    scene_df = pd.DataFrame(scene_df)\n",
    "    scene_df.to_csv(tgt_pth, index=False)\n",
    "    \n",
    "    \n",
    "def train_test_split(cfg):\n",
    "    \n",
    "    tgt_pth = f\"E:\\datasets\\iGibson-dataset\\position_csv/\"\n",
    "    \n",
    "    # Merge train scene csv\n",
    "    train_df = pd.DataFrame()\n",
    "    train_list = cfg[\"train\"]\n",
    "    \n",
    "    for train_idx in train_list:\n",
    "        \n",
    "        scene_id = scene_list[train_idx]\n",
    "        scene_csv = f\"{tgt_pth}/{scene_id}.csv\"\n",
    "        scene_df = pd.read_csv(scene_csv)\n",
    "        \n",
    "        train_df = pd.concat((train_df, scene_df))\n",
    "        \n",
    "    train_df.to_csv(f\"{tgt_pth}/train.csv\", index=False)\n",
    "        \n",
    "    # Merge test scene csv\n",
    "    test_df = pd.DataFrame()\n",
    "    test_list = cfg[\"test\"]\n",
    "    \n",
    "    for test_idx in test_list:\n",
    "        \n",
    "        scene_id = scene_list[test_idx]\n",
    "        scene_csv = f\"{tgt_pth}/{scene_id}.csv\"\n",
    "        scene_df = pd.read_csv(scene_csv)\n",
    "        \n",
    "        test_df = pd.concat((test_df, scene_df))\n",
    "        \n",
    "    test_df.to_csv(f\"{tgt_pth}/test.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beechwood_1_int\n",
      "Ihlen_0_int\n",
      "Ihlen_1_int\n",
      "Merom_0_int\n",
      "Merom_1_int\n"
     ]
    }
   ],
   "source": [
    "# Generate directory csv for each scene\n",
    "s_idx_list = [1, 3, 4, 5, 6]\n",
    "\n",
    "for s_idx in s_idx_list:\n",
    "    \n",
    "    print(scene_list[s_idx])\n",
    "    process_scene_df(scene_list[s_idx])\n",
    "    \n",
    "\n",
    "# Split train and test dataset    \n",
    "cfg = {\n",
    "    \"train\": [\n",
    "        1, 3, 4, 5\n",
    "    ],\n",
    "    \"test\": [\n",
    "        6\n",
    "    ]\n",
    "}\n",
    "\n",
    "train_test_split(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image map and image observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import read_gt_position\n",
    "from utils import read_d_position, read_rgb_position\n",
    "from utils import read_mp_image\n",
    "\n",
    "\n",
    "class PositionIMIO(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_pth):\n",
    "        \n",
    "        super(PositionIMIO, self).__init__()\n",
    "        \n",
    "        self.data = pd.read_csv(data_pth)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Read ground truth\n",
    "        gt_pth = self.data[\"gt\"][index]\n",
    "        gt = read_gt_position(gt_pth)\n",
    "        gt = gt[0, ...]\n",
    "        \n",
    "        # Read observation\n",
    "        # obs_pth = self.data[\"rgb\"][index]\n",
    "        # obs = read_rgb_position(obs_pth)\n",
    "        \n",
    "        obs_pth = self.data[\"d\"][index]\n",
    "        obs = read_d_position(obs_pth)\n",
    "        \n",
    "        # Read map\n",
    "        mp_pth = self.data[\"mp\"][index]\n",
    "        mp = read_mp_image(mp_pth)\n",
    "        \n",
    "        \n",
    "        return (gt, obs, mp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset has 80 samples\n",
      "torch.Size([4])\n",
      "torch.Size([4, 8, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "Test dataset has 20 samples\n",
      "torch.Size([4])\n",
      "torch.Size([4, 8, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_pth = \"e:\\datasets\\iGibson-dataset\\position_csv/train.csv\"\n",
    "train_dataset = PositionIMIO(train_pth)\n",
    "print(f\"Train dataset has {len(train_dataset)} samples\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "gt, obs, mp = next(iter(train_dataloader))\n",
    "print(gt.shape)\n",
    "print(obs.shape)\n",
    "print(mp.shape)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    pass\n",
    "\n",
    "test_pth = \"e:\\datasets\\iGibson-dataset\\position_csv/test.csv\"\n",
    "test_dataset = PositionIMIO(test_pth)\n",
    "print(f\"Test dataset has {len(test_dataset)} samples\")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "gt, obs, mp = next(iter(test_dataloader))\n",
    "print(gt.shape)\n",
    "print(obs.shape)\n",
    "print(mp.shape)\n",
    "for data in train_dataloader:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igibson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
